{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fc506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (1.78.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/maloti12/.local/lib/python3.13/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/maloti12/.local/lib/python3.13/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/maloti12/.local/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/maloti12/.local/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/maloti12/anaconda3/envs/ethics/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/maloti12/.local/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/maloti12/.local/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea60e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95132b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access variables using os.getenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"API Key loaded successfully.\")\n",
    "# Initialize OpenAI API client\n",
    "openai = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3ccd6",
   "metadata": {},
   "source": [
    "## Create the batch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4883ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: choice_with_norm\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/choice_prompts_with_norm_batch.jsonl\n",
      "Processing: choice_without_norm\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/choice_prompts_without_norm_batch.jsonl\n",
      "Processing: immoral_with_norm\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/immoral_act_with_norm_prompts_batch.jsonl\n",
      "Processing: immoral_without_norm\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/immoral_act_without_norm_prompts_batch.jsonl\n",
      "Processing: moral_action_immoral_outcome\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/with_moralAction_immoralConsequnece_prompts_batch.jsonl\n",
      "Processing: immoral_action_moral_outcome\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/with_immoralAction_moralConsequnece_prompts_batch.jsonl\n",
      "Processing: injection_moral_action_immoral_outcome\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/injection_moralAction_immoralOutcome_prompts_batch.jsonl\n",
      "Processing: pro_action_immoral\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/anti_action_immoralAction_prompts_batch.jsonl\n",
      "Processing: pro_outcome_immoral\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/pro_outcome_immoralAction_prompts_batch.jsonl\n",
      "Processing: outcome_weighted_moral\n",
      "Batch file created: /home/maloti12/shreshtho/self/Research/ethics/illusion_of_ethics/data/batch/outcome_weighted_moralAction_prompts_batch.jsonl\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"utils\")))\n",
    "from create_batchFiles_openAi import generate_all_batches\n",
    "question_bank = generate_all_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d94ae394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question bank saved to ../data/info/question_bank.json\n"
     ]
    }
   ],
   "source": [
    "info_dir = os.path.join(\"../data\", \"info\")\n",
    "os.makedirs(info_dir, exist_ok=True)\n",
    "question_bank_path = os.path.join(info_dir, \"question_bank.json\")\n",
    "\n",
    "with open(question_bank_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(question_bank, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Question bank saved to {question_bank_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406529ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_DIR = \"../data/batch\"\n",
    "TASKS_DIR = \"../data/tasks\"\n",
    "QUESTION_BANK_PATH = \"../data/info/question_bank.json\"\n",
    "\n",
    "def load_question_bank(path=QUESTION_BANK_PATH):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_key_from_filename(filename):\n",
    "    return filename.replace(\"_batch.jsonl\", \"\")\n",
    "\n",
    "def create_task_files_for_batches(model=\"gpt-4o-mini\", temperature=0.1):\n",
    "    os.makedirs(TASKS_DIR, exist_ok=True)\n",
    "    question_bank = load_question_bank()\n",
    "\n",
    "    for filename in os.listdir(BATCH_DIR):\n",
    "        if not filename.endswith(\"_batch.jsonl\"):\n",
    "            continue\n",
    "\n",
    "        key = extract_key_from_filename(filename)\n",
    "        question = None\n",
    "\n",
    "        for q_key, value in question_bank.items():\n",
    "            if value[\"file\"] == f\"{key}.jsonl\":\n",
    "                question = value[\"question\"]\n",
    "                break\n",
    "\n",
    "        if question is None:\n",
    "            print(f\"[Warning] No question found for batch file: {filename}\")\n",
    "            continue\n",
    "\n",
    "        batch_file_path = os.path.join(BATCH_DIR, filename)\n",
    "        task_file_path = os.path.join(TASKS_DIR, f\"{key}_tasks.jsonl\")\n",
    "\n",
    "        with open(batch_file_path, 'r', encoding='utf-8') as infile, \\\n",
    "             open(task_file_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "            for idx, line in enumerate(infile):\n",
    "                data = json.loads(line)\n",
    "                prompt_text = data.get(\"prompt\", \"\")\n",
    "                task = {\n",
    "                    \"custom_id\": f\"{key}-task-{idx}\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\n",
    "                        \"model\": model,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"response_format\": { \"type\": \"json_object\" },\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": question},\n",
    "                            {\"role\": \"user\", \"content\": prompt_text}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "                outfile.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "        print(f\"✅ Task file created: {task_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f824ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task file created: ../data/tasks/choice_prompts_with_norm_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/choice_prompts_without_norm_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/immoral_act_with_norm_prompts_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/immoral_act_without_norm_prompts_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/with_moralAction_immoralConsequnece_prompts_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/with_immoralAction_moralConsequnece_prompts_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/injection_moralAction_immoralOutcome_prompts_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/anti_action_immoralAction_prompts_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/pro_outcome_immoralAction_prompts_tasks.jsonl\n",
      "✅ Task file created: ../data/tasks/outcome_weighted_moralAction_prompts_tasks.jsonl\n"
     ]
    }
   ],
   "source": [
    "create_task_files_for_batches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504b9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_all_batch_files(tasks_dir=TASKS_DIR):\n",
    "    batch_file_ids = {}\n",
    "\n",
    "    for file_name in os.listdir(tasks_dir):\n",
    "        if not file_name.endswith(\".jsonl\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(tasks_dir, file_name)\n",
    "\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            uploaded_file = openai.files.create(\n",
    "                file=f,\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "            batch_file_ids[file_name] = uploaded_file.id\n",
    "            print(f\"✅ Uploaded: {file_name} → File ID: {uploaded_file.id}\")\n",
    "\n",
    "    return batch_file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "973166d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: choice_prompts_with_norm_tasks.jsonl → File ID: file-AwnthnRvAwNHNPWQV9tYzo\n",
      "✅ Uploaded: choice_prompts_without_norm_tasks.jsonl → File ID: file-UxhyCJvAo2KYRutkbZaf5d\n",
      "✅ Uploaded: immoral_act_with_norm_prompts_tasks.jsonl → File ID: file-RSPzHMSnYK8ZkxwsvFyNZ7\n",
      "✅ Uploaded: immoral_act_without_norm_prompts_tasks.jsonl → File ID: file-H2v4RGhDCXW8GVu4Aoficz\n",
      "✅ Uploaded: with_moralAction_immoralConsequnece_prompts_tasks.jsonl → File ID: file-SX8DdcV73dYsux6GstsY3Q\n",
      "✅ Uploaded: with_immoralAction_moralConsequnece_prompts_tasks.jsonl → File ID: file-Eb5NHoeWbJgae75oGLCFSD\n",
      "✅ Uploaded: injection_moralAction_immoralOutcome_prompts_tasks.jsonl → File ID: file-DhnGstxrpFDM3d3ccDCe4Q\n",
      "✅ Uploaded: anti_action_immoralAction_prompts_tasks.jsonl → File ID: file-1gf5QVEYpfncieGuNcpJ7C\n",
      "✅ Uploaded: pro_outcome_immoralAction_prompts_tasks.jsonl → File ID: file-MuFdEbqeDgDkwytfC4TQ1U\n",
      "✅ Uploaded: outcome_weighted_moralAction_prompts_tasks.jsonl → File ID: file-B9WC1rmKvB6LqiDc4WWjSj\n",
      "✅ File ID map saved to ../data/file_id_map.json\n"
     ]
    }
   ],
   "source": [
    "file_id_map = upload_all_batch_files()\n",
    "def save_file_id_map(file_id_map, output_path=\"../data/file_id_map.json\"):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(file_id_map, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✅ File ID map saved to {output_path}\")\n",
    "save_file_id_map(file_id_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceef87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILE_ID_MAP_PATH = \"../data/file_id_map.json\"\n",
    "\n",
    "def create_batch_jobs_from_existing_files(tasks_dir=TASKS_DIR, file_id_map_path=FILE_ID_MAP_PATH):\n",
    "    # Load file ID map\n",
    "    with open(file_id_map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_id_map = json.load(f)\n",
    "\n",
    "    batch_jobs = {}\n",
    "\n",
    "    for file_name in os.listdir(tasks_dir):\n",
    "        if not file_name.endswith(\".jsonl\"):\n",
    "            continue\n",
    "\n",
    "        if file_name not in file_id_map:\n",
    "            print(f\"⚠️ Skipping {file_name} — not found in file_id_map.\")\n",
    "            continue\n",
    "\n",
    "        file_id = file_id_map[file_name]\n",
    "\n",
    "        # Create batch job\n",
    "        batch_job = openai.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"🚀 Batch job submitted for {file_name} → Job ID: {batch_job.id}\")\n",
    "\n",
    "        batch_jobs[file_name] = {\n",
    "            \"file_id\": file_id,\n",
    "            \"batch_job_id\": batch_job.id,\n",
    "            \"status\": batch_job.status\n",
    "        }\n",
    "\n",
    "    return batch_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd58036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Batch job submitted for choice_prompts_with_norm_tasks.jsonl → Job ID: batch_6820e081b6a48190a2089a2a1a52dd28\n",
      "🚀 Batch job submitted for choice_prompts_without_norm_tasks.jsonl → Job ID: batch_6820e0822c68819090fdee2646c5ce80\n",
      "🚀 Batch job submitted for immoral_act_with_norm_prompts_tasks.jsonl → Job ID: batch_6820e082c5708190a75bf2abd17db752\n",
      "🚀 Batch job submitted for immoral_act_without_norm_prompts_tasks.jsonl → Job ID: batch_6820e0834ebc8190b119eeb79976325a\n",
      "🚀 Batch job submitted for with_moralAction_immoralConsequnece_prompts_tasks.jsonl → Job ID: batch_6820e083c0948190a1911d7e7df0d769\n",
      "🚀 Batch job submitted for with_immoralAction_moralConsequnece_prompts_tasks.jsonl → Job ID: batch_6820e08bbc0c8190a553670997ad26e5\n",
      "🚀 Batch job submitted for injection_moralAction_immoralOutcome_prompts_tasks.jsonl → Job ID: batch_6820e08c46b48190a6211e34a0f1e936\n",
      "🚀 Batch job submitted for anti_action_immoralAction_prompts_tasks.jsonl → Job ID: batch_6820e08caaf48190b87734efac4d5def\n",
      "🚀 Batch job submitted for pro_outcome_immoralAction_prompts_tasks.jsonl → Job ID: batch_6820e08d0de881908fa7157aa8061163\n",
      "🚀 Batch job submitted for outcome_weighted_moralAction_prompts_tasks.jsonl → Job ID: batch_6820e08d74808190ba2207eb48bbc011\n",
      "✅ Job info saved to ../data/job_info.json\n"
     ]
    }
   ],
   "source": [
    "job_info = create_batch_jobs_from_existing_files()\n",
    "def save_job_info(job_info, output_path=\"../data/job_info.json\"):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(job_info, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✅ Job info saved to {output_path}\")\n",
    "save_job_info(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3030e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_INFO_PATH = \"../data/job_info.json\"\n",
    "\n",
    "def check_job_status(job_id):\n",
    "    try:\n",
    "        job_info = openai.batches.retrieve(job_id)\n",
    "        return job_info\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error retrieving job status for {job_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_all_jobs_status(job_info_path=JOB_INFO_PATH):\n",
    "    with open(job_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        job_info = json.load(f)\n",
    "\n",
    "    for file_name, info in job_info.items():\n",
    "        job_id = info.get(\"batch_job_id\")\n",
    "        status = check_job_status(job_id)\n",
    "        if status:\n",
    "            print(f\"📄 {file_name} → Job ID: {job_id}, Status: {status.status}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Could not retrieve status for {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b6281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 choice_prompts_with_norm_tasks.jsonl → Job ID: batch_6820e081b6a48190a2089a2a1a52dd28, Status: in_progress\n",
      "📄 choice_prompts_without_norm_tasks.jsonl → Job ID: batch_6820e0822c68819090fdee2646c5ce80, Status: failed\n",
      "📄 immoral_act_with_norm_prompts_tasks.jsonl → Job ID: batch_6820e082c5708190a75bf2abd17db752, Status: failed\n",
      "📄 immoral_act_without_norm_prompts_tasks.jsonl → Job ID: batch_6820e0834ebc8190b119eeb79976325a, Status: failed\n",
      "📄 with_moralAction_immoralConsequnece_prompts_tasks.jsonl → Job ID: batch_6820e083c0948190a1911d7e7df0d769, Status: failed\n",
      "📄 with_immoralAction_moralConsequnece_prompts_tasks.jsonl → Job ID: batch_6820e08bbc0c8190a553670997ad26e5, Status: failed\n",
      "📄 injection_moralAction_immoralOutcome_prompts_tasks.jsonl → Job ID: batch_6820e08c46b48190a6211e34a0f1e936, Status: failed\n",
      "📄 anti_action_immoralAction_prompts_tasks.jsonl → Job ID: batch_6820e08caaf48190b87734efac4d5def, Status: failed\n",
      "📄 pro_outcome_immoralAction_prompts_tasks.jsonl → Job ID: batch_6820e08d0de881908fa7157aa8061163, Status: failed\n",
      "📄 outcome_weighted_moralAction_prompts_tasks.jsonl → Job ID: batch_6820e08d74808190ba2207eb48bbc011, Status: failed\n"
     ]
    }
   ],
   "source": [
    "check_all_jobs_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af038dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch_errors(job_id):\n",
    "    try:\n",
    "        return openai.batches.retrieve(batch_id=job_id).errors\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to retrieve errors for job {job_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: check one failed job\n",
    "get_batch_errors(\"batch_6820e0822c68819090fdee2646c5ce80\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7059e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6820e081b6a48190a2089a2a1a52dd28', completion_window='24h', created_at=1746985089, endpoint='/v1/chat/completions', input_file_id='file-AwnthnRvAwNHNPWQV9tYzo', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747071489, failed_at=None, finalizing_at=None, in_progress_at=1746985092, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=11947, total=12000))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a non-empty value for `file_id` but received None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m result_file_id = batch_job.output_file_id\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Get result content\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m result = openai.files.content(result_file_id).content\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Save to a file\u001b[39;00m\n\u001b[32m     15\u001b[39m result_file_name = \u001b[33m\"\u001b[39m\u001b[33m../data/batch_job_results_choice_with_norm.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ethics/lib/python3.13/site-packages/openai/resources/files.py:279\u001b[39m, in \u001b[36mFiles.content\u001b[39m\u001b[34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03mReturns the contents of the specified file.\u001b[39;00m\n\u001b[32m    268\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_id:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a non-empty value for `file_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/binary\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get(\n\u001b[32m    282\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/files/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/content\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    283\u001b[39m     options=make_request_options(\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m     cast_to=_legacy_response.HttpxBinaryResponseContent,\n\u001b[32m    287\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Expected a non-empty value for `file_id` but received None"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "batch_id = \"batch_6820e081b6a48190a2089a2a1a52dd28\"\n",
    "\n",
    "\n",
    "batch_job = openai.batches.retrieve(batch_id)\n",
    "result_file_id = batch_job.output_file_id\n",
    "\n",
    "# Get result content\n",
    "result = openai.files.content(result_file_id).content\n",
    "\n",
    "# Save to a file\n",
    "result_file_name = \"../data/batch_job_results_choice_with_norm.jsonl\"\n",
    "with open(result_file_name, 'wb') as file:\n",
    "    file.write(result)\n",
    "\n",
    "# Load and parse results\n",
    "results = []\n",
    "with open(result_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        results.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"✅ Loaded {len(results)} results from {result_file_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethics-kernal",
   "language": "python",
   "name": "ethics-kernal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
