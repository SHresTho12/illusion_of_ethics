{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fc506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (1.78.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: colorama in d:\\shreshtho\\research\\ethics\\illusion_of_ethics\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea60e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95132b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully.\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAPI Key loaded successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Initialize OpenAI API client\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m openai = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\venv\\Lib\\site-packages\\openai\\_client.py:124\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    122\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    125\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access variables using os.getenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"API Key loaded successfully.\")\n",
    "# Initialize OpenAI API client\n",
    "openai = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3ccd6",
   "metadata": {},
   "source": [
    "## Create the batch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4883ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: choice_with_norm\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\choice_prompts_with_norm_batch.jsonl\n",
      "Processing: choice_without_norm\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\choice_prompts_without_norm_batch.jsonl\n",
      "Processing: immoral_with_norm\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\immoral_act_with_norm_prompts_batch.jsonl\n",
      "Processing: immoral_without_norm\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\immoral_act_without_norm_prompts_batch.jsonl\n",
      "Processing: moral_action_immoral_outcome\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\with_moralAction_immoralConsequnece_base_prompts_batch.jsonl\n",
      "Processing: immoral_action_moral_outcome\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\with_immoralAction_moralConsequnece_base_prompts_batch.jsonl\n",
      "Processing: injection_moral_action_immoral_outcome\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\injection_moralAction_immoralOutcome_prompts_batch.jsonl\n",
      "Processing: pro_action_immoral\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\anti_action_immoralAction_prompts_batch.jsonl\n",
      "Processing: pro_outcome_immoral\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\pro_outcome_immoralAction_prompts_batch.jsonl\n",
      "Processing: outcome_weighted_moral\n",
      "Batch file created: d:\\Shreshtho\\Research\\Ethics\\illusion_of_ethics\\data\\batch\\outcome_weighted_moralAction_prompts_batch.jsonl\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"utils\")))\n",
    "from create_batchFiles_openAi import generate_all_batches\n",
    "question_bank = generate_all_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94ae394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question bank saved to ../data\\info\\question_bank.json\n"
     ]
    }
   ],
   "source": [
    "info_dir = os.path.join(\"../data\", \"info\")\n",
    "os.makedirs(info_dir, exist_ok=True)\n",
    "question_bank_path = os.path.join(info_dir, \"question_bank.json\")\n",
    "\n",
    "with open(question_bank_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(question_bank, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Question bank saved to {question_bank_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406529ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_DIR = \"../data/batch\"\n",
    "TASKS_DIR = \"../data/tasks\"\n",
    "QUESTION_BANK_PATH = \"../data/info/question_bank.json\"\n",
    "\n",
    "def load_question_bank(path=QUESTION_BANK_PATH):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_key_from_filename(filename):\n",
    "    return filename.replace(\"_batch.jsonl\", \"\")\n",
    "\n",
    "def create_task_files_for_batches(model=\"gpt-4o-mini\", temperature=0.1):\n",
    "    os.makedirs(TASKS_DIR, exist_ok=True)\n",
    "    question_bank = load_question_bank()\n",
    "\n",
    "    for filename in os.listdir(BATCH_DIR):\n",
    "        if not filename.endswith(\"_batch.jsonl\"):\n",
    "            continue\n",
    "\n",
    "        key = extract_key_from_filename(filename)\n",
    "        question = None\n",
    "\n",
    "        for q_key, value in question_bank.items():\n",
    "            if value[\"file\"] == f\"{key}.jsonl\":\n",
    "                question = value[\"question\"]\n",
    "                break\n",
    "\n",
    "        if question is None:\n",
    "            print(f\"[Warning] No question found for batch file: {filename}\")\n",
    "            continue\n",
    "\n",
    "        batch_file_path = os.path.join(BATCH_DIR, filename)\n",
    "        task_file_path = os.path.join(TASKS_DIR, f\"{key}_tasks.jsonl\")\n",
    "\n",
    "        with open(batch_file_path, 'r', encoding='utf-8') as infile, \\\n",
    "             open(task_file_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "            for idx, line in enumerate(infile):\n",
    "                data = json.loads(line)\n",
    "                prompt_text = data.get(\"prompt\", \"\")\n",
    "                task = {\n",
    "                    \"custom_id\": f\"{key}-task-{idx}\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\n",
    "                        \"model\": model,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"response_format\": { \"type\": \"json_object\" },\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": question},\n",
    "                            {\"role\": \"user\", \"content\": prompt_text}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "                outfile.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "        print(f\"âœ… Task file created: {task_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f824ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Task file created: ../data/tasks\\anti_action_immoralAction_prompts_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\choice_prompts_without_norm_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\choice_prompts_with_norm_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\immoral_act_without_norm_prompts_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\immoral_act_with_norm_prompts_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\injection_moralAction_immoralOutcome_prompts_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\outcome_weighted_moralAction_prompts_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\pro_outcome_immoralAction_prompts_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\with_immoralAction_moralConsequnece_base_prompts_tasks.jsonl\n",
      "âœ… Task file created: ../data/tasks\\with_moralAction_immoralConsequnece_base_prompts_tasks.jsonl\n"
     ]
    }
   ],
   "source": [
    "create_task_files_for_batches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "504b9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_all_batch_files(tasks_dir=TASKS_DIR):\n",
    "    batch_file_ids = {}\n",
    "\n",
    "    for file_name in os.listdir(tasks_dir):\n",
    "        if not file_name.endswith(\".jsonl\"):\n",
    "            continue\n",
    "        ## only up the files starts with choice\n",
    "        if not file_name.startswith(\"choice\"):\n",
    "            continue\n",
    "        file_path = os.path.join(tasks_dir, file_name)\n",
    "\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            uploaded_file = openai.files.create(\n",
    "                file=f,\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "            batch_file_ids[file_name] = uploaded_file.id\n",
    "            print(f\"âœ… Uploaded: {file_name} â†’ File ID: {uploaded_file.id}\")\n",
    "\n",
    "    return batch_file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "973166d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Uploaded: choice_prompts_with_norm_tasks.jsonl â†’ File ID: file-1kva4C47BN1okCNk8ABqGQ\n",
      "âœ… Uploaded: choice_prompts_without_norm_tasks.jsonl â†’ File ID: file-KfpUKDDHgA89eX2NHbwtdc\n",
      "âœ… File ID map saved to ../data/file_id_map.json\n"
     ]
    }
   ],
   "source": [
    "file_id_map = upload_all_batch_files()\n",
    "def save_file_id_map(file_id_map, output_path=\"../data/file_id_map.json\"):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(file_id_map, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… File ID map saved to {output_path}\")\n",
    "save_file_id_map(file_id_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceef87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILE_ID_MAP_PATH = \"../data/file_id_map.json\"\n",
    "\n",
    "def create_batch_jobs_from_existing_files(tasks_dir=TASKS_DIR, file_id_map_path=FILE_ID_MAP_PATH):\n",
    "    # Load file ID map\n",
    "    with open(file_id_map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_id_map = json.load(f)\n",
    "\n",
    "    batch_jobs = {}\n",
    "\n",
    "    for file_name in os.listdir(tasks_dir):\n",
    "        if not file_name.endswith(\".jsonl\"):\n",
    "            continue\n",
    "\n",
    "        if file_name not in file_id_map:\n",
    "            print(f\"âš ï¸ Skipping {file_name} â€” not found in file_id_map.\")\n",
    "            continue\n",
    "\n",
    "        file_id = file_id_map[file_name]\n",
    "\n",
    "        # Create batch job\n",
    "        batch_job = openai.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"ðŸš€ Batch job submitted for {file_name} â†’ Job ID: {batch_job.id}\")\n",
    "\n",
    "        batch_jobs[file_name] = {\n",
    "            \"file_id\": file_id,\n",
    "            \"batch_job_id\": batch_job.id,\n",
    "            \"status\": batch_job.status\n",
    "        }\n",
    "\n",
    "    return batch_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd58036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Batch job submitted for choice_prompts_with_norm_tasks.jsonl â†’ Job ID: batch_68216cf49c14819083a53bd7c74246ea\n",
      "ðŸš€ Batch job submitted for choice_prompts_without_norm_tasks.jsonl â†’ Job ID: batch_68216cf5176881909f10dcc18efb5d76\n",
      "âš ï¸ Skipping immoral_act_with_norm_prompts_tasks.jsonl â€” not found in file_id_map.\n",
      "âš ï¸ Skipping immoral_act_without_norm_prompts_tasks.jsonl â€” not found in file_id_map.\n",
      "âš ï¸ Skipping with_moralAction_immoralConsequnece_prompts_tasks.jsonl â€” not found in file_id_map.\n",
      "âš ï¸ Skipping with_immoralAction_moralConsequnece_prompts_tasks.jsonl â€” not found in file_id_map.\n",
      "âš ï¸ Skipping injection_moralAction_immoralOutcome_prompts_tasks.jsonl â€” not found in file_id_map.\n",
      "âš ï¸ Skipping anti_action_immoralAction_prompts_tasks.jsonl â€” not found in file_id_map.\n",
      "âš ï¸ Skipping pro_outcome_immoralAction_prompts_tasks.jsonl â€” not found in file_id_map.\n",
      "âš ï¸ Skipping outcome_weighted_moralAction_prompts_tasks.jsonl â€” not found in file_id_map.\n",
      "âœ… Job info saved to ../data/job_info.json\n"
     ]
    }
   ],
   "source": [
    "job_info = create_batch_jobs_from_existing_files()\n",
    "def save_job_info(job_info, output_path=\"../data/job_info.json\"):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(job_info, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… Job info saved to {output_path}\")\n",
    "save_job_info(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3030e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_INFO_PATH = \"../data/job_info.json\"\n",
    "\n",
    "def check_job_status(job_id):\n",
    "    try:\n",
    "        job_info = openai.batches.retrieve(job_id)\n",
    "        return job_info\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error retrieving job status for {job_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_all_jobs_status(job_info_path=JOB_INFO_PATH):\n",
    "    with open(job_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        job_info = json.load(f)\n",
    "\n",
    "    for file_name, info in job_info.items():\n",
    "        job_id = info.get(\"batch_job_id\")\n",
    "        status = check_job_status(job_id)\n",
    "        if status:\n",
    "            print(f\"ðŸ“„ {file_name} â†’ Job ID: {job_id}, Status: {status.status}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Could not retrieve status for {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b6281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ choice_prompts_with_norm_tasks.jsonl â†’ Job ID: batch_68216cf49c14819083a53bd7c74246ea, Status: failed\n",
      "ðŸ“„ choice_prompts_without_norm_tasks.jsonl â†’ Job ID: batch_68216cf5176881909f10dcc18efb5d76, Status: in_progress\n"
     ]
    }
   ],
   "source": [
    "check_all_jobs_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af038dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch_errors(job_id):\n",
    "    try:\n",
    "        return openai.batches.retrieve(batch_id=job_id).errors\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to retrieve errors for job {job_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: check one failed job\n",
    "get_batch_errors(\"batch_68216cf49c14819083a53bd7c74246ea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7059e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6820e081b6a48190a2089a2a1a52dd28', completion_window='24h', created_at=1746985089, endpoint='/v1/chat/completions', input_file_id='file-AwnthnRvAwNHNPWQV9tYzo', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747071489, failed_at=None, finalizing_at=None, in_progress_at=1746985092, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=11947, total=12000))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a non-empty value for `file_id` but received None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m result_file_id = batch_job.output_file_id\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Get result content\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m result = openai.files.content(result_file_id).content\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Save to a file\u001b[39;00m\n\u001b[32m     15\u001b[39m result_file_name = \u001b[33m\"\u001b[39m\u001b[33m../data/batch_job_results_choice_with_norm.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ethics/lib/python3.13/site-packages/openai/resources/files.py:279\u001b[39m, in \u001b[36mFiles.content\u001b[39m\u001b[34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03mReturns the contents of the specified file.\u001b[39;00m\n\u001b[32m    268\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_id:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a non-empty value for `file_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/binary\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get(\n\u001b[32m    282\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/files/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/content\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    283\u001b[39m     options=make_request_options(\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m     cast_to=_legacy_response.HttpxBinaryResponseContent,\n\u001b[32m    287\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Expected a non-empty value for `file_id` but received None"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "batch_id = \"batch_6820e081b6a48190a2089a2a1a52dd28\"\n",
    "\n",
    "\n",
    "batch_job = openai.batches.retrieve(batch_id)\n",
    "result_file_id = batch_job.output_file_id\n",
    "\n",
    "# Get result content\n",
    "result = openai.files.content(result_file_id).content\n",
    "\n",
    "# Save to a file\n",
    "result_file_name = \"../data/batch_job_results_choice_with_norm.jsonl\"\n",
    "with open(result_file_name, 'wb') as file:\n",
    "    file.write(result)\n",
    "\n",
    "# Load and parse results\n",
    "results = []\n",
    "with open(result_file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        results.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"âœ… Loaded {len(results)} results from {result_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69dfc725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[Batch](data=[Batch(id='batch_6820e08d74808190ba2207eb48bbc011', completion_window='24h', created_at=1746985101, endpoint='/v1/chat/completions', input_file_id='file-B9WC1rmKvB6LqiDc4WWjSj', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071501, failed_at=1746985106, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e08d0de881908fa7157aa8061163', completion_window='24h', created_at=1746985101, endpoint='/v1/chat/completions', input_file_id='file-MuFdEbqeDgDkwytfC4TQ1U', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071501, failed_at=1746985103, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e08caaf48190b87734efac4d5def', completion_window='24h', created_at=1746985100, endpoint='/v1/chat/completions', input_file_id='file-1gf5QVEYpfncieGuNcpJ7C', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071500, failed_at=1746985105, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e08c46b48190a6211e34a0f1e936', completion_window='24h', created_at=1746985100, endpoint='/v1/chat/completions', input_file_id='file-DhnGstxrpFDM3d3ccDCe4Q', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071500, failed_at=1746985103, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e08bbc0c8190a553670997ad26e5', completion_window='24h', created_at=1746985099, endpoint='/v1/chat/completions', input_file_id='file-Eb5NHoeWbJgae75oGLCFSD', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071499, failed_at=1746985103, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e083c0948190a1911d7e7df0d769', completion_window='24h', created_at=1746985091, endpoint='/v1/chat/completions', input_file_id='file-SX8DdcV73dYsux6GstsY3Q', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071491, failed_at=1746985160, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e0834ebc8190b119eeb79976325a', completion_window='24h', created_at=1746985091, endpoint='/v1/chat/completions', input_file_id='file-H2v4RGhDCXW8GVu4Aoficz', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071491, failed_at=1746985095, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e082c5708190a75bf2abd17db752', completion_window='24h', created_at=1746985090, endpoint='/v1/chat/completions', input_file_id='file-RSPzHMSnYK8ZkxwsvFyNZ7', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071490, failed_at=1746985094, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e0822c68819090fdee2646c5ce80', completion_window='24h', created_at=1746985090, endpoint='/v1/chat/completions', input_file_id='file-UxhyCJvAo2KYRutkbZaf5d', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-M75PpyAXTAENv6t6agrahFM4. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1747071490, failed_at=1746985094, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_6820e081b6a48190a2089a2a1a52dd28', completion_window='24h', created_at=1746985089, endpoint='/v1/chat/completions', input_file_id='file-AwnthnRvAwNHNPWQV9tYzo', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1746988320, error_file_id='file-7f7uY6xtWgWL3yc8t9nbgD', errors=None, expired_at=None, expires_at=1747071489, failed_at=None, finalizing_at=1746987294, in_progress_at=1746985092, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=12000, total=12000))], has_more=False, object='list', first_id='batch_6820e08d74808190ba2207eb48bbc011', last_id='batch_6820e081b6a48190a2089a2a1a52dd28')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "openai.batches.list(limit=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
